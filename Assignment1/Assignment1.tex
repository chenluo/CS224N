\documentclass[11pt, oneside]{article}   	% use "amsart" instead of "article" for AMSLaTeX format
\usepackage{geometry}                		% See geometry.pdf to learn the layout options. There are lots.
\geometry{letterpaper}                   		% ... or a4paper or a5paper or ... 
%\geometry{landscape}                		% Activate for rotated page geometry
%\usepackage[parfill]{parskip}    		% Activate to begin paragraphs with an empty line rather than an indent
\usepackage{graphicx}				% Use pdf, png, jpg, or eps§ with pdflatex; use eps in DVI mode
								% TeX will automatically convert eps --> pdf in pdflatex		
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{bm}



%SetFonts

%SetFonts


\title{Assignment 1}
\author{Chen Luo}
%\date{}							% Activate to display a given date or no date

\begin{document}
\maketitle
\section{Softmax}
\subsection{a}
\begin{align}
softmax(\bm{x} + c) &=  \frac{e^{\bm{x+c}}}{\sum_{j = 1}^{d}{e^{x_j+c}}}\\
&= \frac{e^{\bm{x}} e^c}{e^c \sum_{j=1}^{d}{e^{x_j}}} = softmax(\bm{x})
\end{align}

\section{NN Basic}
\subsection{a}
\begin{align}
\frac{\partial\sigma(x)}{\partial x} &= -1(1+e^{-x})^{-2}\frac{\partial(1+e^{-x})}{\partial x}\\
&= \frac{e^{-x}}{(1+e^{-x})^{2}}\\
&= \frac{e^{-x}+1-1}{(1+e^{-x})^{2}}\\
&= \sigma(x)-\sigma(x)^2
\end{align}

\subsection{b}
\begin{align}
CE(\bm{y}, \hat{\bm{y}}) &= -\sum_i y_i{\log (\hat{y_i})}\\
\hat{\bm{y}} &= softmax(\bm{\theta}) = \frac{e^{\bm{\theta}}}{\sum_j{e^{\theta_j}}}\\
\frac{\partial CE(\bm{y}, \hat{\bm{y}})}{\partial \bm\theta} &= -\frac{\partial \sum_i 1_{[y_i==1]}\left\{\log{e^{\theta_i}}-\log{\sum_j {e^{\theta_j}}}\right\}}{\partial \bm{\theta}}\\
&= -\frac{\partial \bm{y}^T {\bm{\theta}} - log\sum_j e^{\theta_j}}{\partial \bm{\theta}}\\
&= -\bm{y} + \frac{\log \bm{1}^T e^{\bm{\theta}}}{\partial \bm{\theta}}\\
&= -\bm{y} + \frac{e^{\bm{\theta}}}{\bm{1}^T e^{\bm{\theta}}}\\
&= -\bm{y} + softmax(\bm{\theta})\\
&= \hat{\bm{y}} - \bm{y}
\end{align}

\subsection{c}
\begin{align}
J = CE(\bm{y}, \hat{\bm{y}}) &= CE(\bm{y}, softmax(\bm{h}\bm{W}_2+\bm{b}_2))\\
&= CE(\bm{y}, softmax(\sigma(\bm{x}\bm{W}_1+\bm{b}_1)\bm{W}_2+\bm{b}_2))
\end{align}

Actually, 
\begin{align}
J = - \bm{y}_{1\times D_y} {\log (\bm{h}_{1\times H}\bm{W}_{H \times D_y}+\bm{b}_{1\times D_y})}^T
\end{align}•
\begin{align}
\frac{\partial J}{\partial \bm{x}} &=\frac{\partial J}{\partial (\bm{h}\bm{W}_2+\bm{b}_2)}\frac{\partial (\bm{h}\bm{W}_2+\bm{b}_2)}{\partial \bm{h}}\frac{\partial \bm{h}}{\partial \bm{x}}\\
&= (\hat{\bm{y}} - \bm{y})\bm{W}_2^T\sigma(\bm{x}\bm{W}_1+\bm{b}_1)\{\bm{1} - \sigma(\bm{x}\bm{W}_1+\bm{b}_1)\}^T\bm{W}_1^T
\end{align},
where $\frac{\partial (\bm{h}\bm{W}_2+\bm{b}_2)}{\partial \bm{h}}$ is Jaccobian matrix.

Or, [TODO]
\begin{align}
d(\bm{h}\bm{W}_2+\bm{b}_2) &= d\bm{h}\bm{W}_2+\bm{h}d\bm{W}_2+d\bm{b}_2\\
vec(d(\bm{h}\bm{W}_2+\bm{b}_2)) &= vec(d\bm{h}\bm{W}_2+\bm{h}d\bm{W}_2+d\bm{b}_2)\\
&= vec(\bm{1}d\bm{h}\bm{W}_2+\bm{h}d\bm{W}_2\bm{I}+\bm{1}d\bm{b}_2\bm{I}) \to preparation for\\
& vec(AXB) = (B^T\otimes A)vec(X)\\
&= \bm{W}_2^T\otimes 1 vec(d\bm{h}) + \bm{I}\otimes \bm{h} vec(d\bm{W}_2) +\bm{I}\otimes 1 vec( d\bm{b}_2)
\end{align}

\subsection{d}
\begin{align}
\bm{W}_1&: D_x \times H\\
\bm{b}_1&: 1 \times H\\
\bm{W}_2 &: H \times D_y\\
\bm{b}_2 &: 1 \times D_y\\
&H(D_x+D_y+1)+D_y
\end{align}

\subsection{g}
\begin{align}
d J &= (\hat{\bm{y}} - \bm{y})^T d(\bm{h}\bm{W}_2+\bm{b}_2) = (\hat{\bm{y}} - \bm{y})^T (d\bm{h}\bm{W}_2+\bm{h}d\bm{W}_2+d\bm{b}_2)\\
&= (\hat{\bm{y}} - \bm{y})^T d\bm{h}\bm{W}_2+(\hat{\bm{y}} - \bm{y})^T \bm{h}d\bm{W}_2+(\hat{\bm{y}} - \bm{y})^T d\bm{b}_2)\\
&= trace((\hat{\bm{y}} - \bm{y})^T d\bm{h}\bm{W}_2+(\hat{\bm{y}} - \bm{y})^T \bm{h}d\bm{W}_2+(\hat{\bm{y}} - \bm{y})^T d\bm{b}_2))\\
\frac{\partial J}{\partial \bm{W}_2} &= (\bm{W}_2(\hat{\bm{y}} - \bm{y})^T)^T\\
&= \bm{h}^T(\hat{\bm{y}} - \bm{y})\\
\frac{\partial J}{\partial \bm{b}_2} &= \hat{\bm{y}} - \bm{y}\\
d \bm{h} &= d(\sigma(\bm{x}\bm{W}_1 + \bm{b}_1)) = {\sigma^\prime}(\bm{\cdot}) \odot (\bm{x}d\bm{W}_1 + d\bm{b}_1)\\
d J &= trace( (\hat{\bm{y}}-\bm{y})^T \sigma^\prime (\cdot) \odot(\bm{x}d\bm{W}_1 +d \bm{b}_1)\bm{W}_2)\\
&= trace(\bm{W}_2(\hat{\bm{y}}-\bm{y})^T \sigma^\prime (\cdot)\odot (\bm{x}d\bm{W}_1 +d \bm{b}_1))\\
& Apply~law~of~trace~and~element-wise~product:\\
& trace(A^T (B\odot C)) = trace((A \odot B)^T C) \\
&= trace([(\hat{\bm{y}}-\bm{y})\bm{W}_2^T \odot \sigma^\prime(\cdot)]^T (\bm{x}d\bm{W}_1 +d \bm{b}_1) )\\
\frac{\partial J}{\partial \bm{W}_1} &= \{[(\hat{\bm{y}}-\bm{y})\bm{W}_2^T \odot \sigma^\prime(\cdot)]^T \bm{x}\}^T\\
&= \bm{x}^T [(\hat{\bm{y}}-\bm{y})\bm{W}_2^T \odot \sigma^\prime(\cdot)]\\
\frac{\partial J}{\partial \bm{b}_1} &= [(\hat{\bm{y}}-\bm{y})\bm{W}_2^T \odot \sigma^\prime(\cdot)]
\end{align}

\section{word2vec}
Notice: $\bm\nu$ and $\bm\mu$ are column vectors. Previously, all vectors are row vectors. 
\subsection{a}
\begin{align}
J_{softmax-CE}(o, \bm{\nu}_c, \bm{U}) &= CE(\bm{y}, \hat{\bm{y}})\\
\hat{\bm{y}} &= softmax(\{\bm{U}_{\{d\times V\}}^T \times \bm{\nu}_{c\{d\times 1\}}\}^T)\\
d CE(\cdot) &= (\hat{\bm{y}} - \bm{y}) d \bm{\nu}_c^T \bm{U}\\
&= trace[(\hat{\bm{y}} - \bm{y}) d \bm{\nu}_c^T \bm{U}]\\
&= trace[ \bm{U}(\hat{\bm{y}} - \bm{y})d \bm{\nu}_c^T]\\
\frac{\partial CE(\cdot)}{\partial \bm{\nu}_c^T} &= (\hat{\bm{y}} - \bm{y})^T\bm{U}^T\\
\frac{\partial CE(\cdot)}{\partial \bm{\nu}_c} &= \bm{U}(\hat{\bm{y}} - \bm{y})
\end{align}

\subsection{b}
\begin{align}
d CE(\cdot) &= (\hat{\bm{y}} - \bm{y}) \bm{\nu}_c^T d\bm{U}\\
&= trace[(\hat{\bm{y}} - \bm{y}) \bm{\nu}_c^T d\bm{U}]\\
\frac{\partial CE}{\partial \bm{U}} &= \bm{\nu}_c(\hat{\bm{y}} - \bm{y})^T
\end{align}

\subsection{c}
Assume that for $\bm{\nu}_c$, $\bm{k}_{neg}$ is a row vector of $K$-hot (ensure $\bm{k}_{neg\{o\}} \neq 1$) and $\bm{k}$ is one-hot vector that $\bm{k}_o = 1$. Rewrite $J_{neg-sample}(\cdot)$ as follow,
\begin{align}
J_{neg-sample}(\cdot) &= -\log (\sigma(\bm{\nu}_c^T\bm{U}))\bm{k}^T - \log(\sigma(-\bm{\nu}_c^T\bm{U}))\bm{k}_{neg}^T\\
&= -\log(\sigma(\bm{\nu}_c^T\bm{U}\bm{k}^T)) - \log(\sigma(-\bm{\nu}_c^T\bm{U}\bm{k}_{neg}^T))
\end{align}
In the last equation, we put all vectors into element-wise operator.
\begin{align}
d J_{neg-sample}(\cdot) &= \frac{\sigma(\bm{\nu}_c^T\bm{U}\bm{k}^T)(1-\sigma(\bm{\nu}_c^T\bm{U}\bm{k}^T))}{\sigma(\bm{\nu}_c^T\bm{U}\bm{k}^T)}\\
\frac{\partial J_{neg-sample}(\cdot)}{\partial \bm\nu_c} &= 
\end{align}
TODO

\begin{align}
\frac{\partial J}{\partial \bm{\nu}_c} &= -\bm\mu_o\frac{\sigma(\bm{\mu}_o^T\bm\nu_c)(1-\sigma(\bm{\mu}_o^T\bm\nu_c))}{\sigma(\bm{\mu}_o^T\bm\nu_c)} + \sum_{k = 1}^K \bm\mu_k(1 - \sigma(-\bm\mu_k^T\bm\nu_c))\\
&= -\bm\mu_o(1-\sigma(\bm{\mu}_o^T\bm\nu_c)) + \sum_{k = 1}^K \bm\mu_k(1 - (1-\sigma(\bm\mu_k^T\bm\nu_c))\\
&= -\bm\mu_o(1-\sigma(\bm{\mu}_o^T\bm\nu_c)) +\sum_{k = 1}^K \bm\mu_k(\sigma(\bm\mu_k^T\bm\nu_c))
\end{align}
\begin{align}
d J(\cdot) &= - \frac{d(\sigma(\cdot))}{\sigma(\cdot)} +\sum_{k = 1}^{K}(1 - \sigma(-\bm\mu_k^T\bm\nu_c))d\bm\mu_k^T\bm\nu_c\\
&=- trace({(1-\sigma(\bm\mu_o^T \bm\nu_c)) d \bm\mu_o^T \bm\nu_c}) \\
&= trace(\bm\nu_c(\sigma(\bm\mu_o^T \bm\nu_c)-1)d \bm\mu_o^T)\\
\frac{\partial J}{\partial \bm{\mu}_o} &= \bm\nu_c(\sigma(\bm\mu_o^T \bm\nu_c)-1)\\
\end{align}
\begin{align}
\frac{\partial J}{\partial \bm{\mu}_k} &= \bm\nu_c\sigma(\bm\mu_k^T\bm\nu_c), for~k = [1, 2, ..., K]
\end{align}

Computation efficiency of negtive-sampling loss, speed up ratio is approximately
$$\frac{V}{K+1}$$

\subsection{d}
For skip-gram: 

As the definition of word-vector, there are no items of $\bm\nu_k$ where $k \neq c$ in cost function $J(o, \bm\nu_c, \bm{U})$ or $F(\cdot)$.

\begin{align}
\frac{\partial J_{skip-gram}(\omega_{t-m},...\omega_{t+m})}{\partial \bm\nu_k} &= \bm{0}, for~any~k\neq c\\
\end{align}
For the remaining parts is only applying the gradients derived by previous questions into the sum operator. The inner parts depend on the specified definition of $F(o, c)$.

For CBOW:

If softmax-cross-entropy cost, 
\begin{align}
p(\omega_t, \hat{\bm{\nu}}) &= \frac{\exp(\bm\mu_{\omega_t}^T \sum_{-m\leq j\leq m, j\neq 0}\bm\nu_{\omega_j})}{\sum_{v = 1}^V\exp(\bm\mu_v^T \sum_{-m\leq j\leq m, j\neq 0}\bm\nu_{\omega_j})}\\
J_{CBOW} = F(\omega_t, \hat{\bm{\nu}}) &= CE(\bm{y}, \hat{\bm{y}} = softmax(\bm{U}_{\{d\times V\}}^T \times \hat{\bm{\nu}}_{\{d\times 1\}}^T))\\
d J_{CBOW} &= \bm{U}(\hat{\bm{y}} - \bm{y}) d \hat{\bm{\nu}}\\
&= \sum_{-m\leq j\leq m, j\neq 0} \bm{U}(\hat{\bm{y}} - \bm{y}) d \bm\nu_{\omega_j}
\end{align}










\end{document}  